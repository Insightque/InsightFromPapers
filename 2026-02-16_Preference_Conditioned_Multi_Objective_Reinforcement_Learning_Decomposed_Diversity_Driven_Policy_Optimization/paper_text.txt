DEBUG: Processing 23 pages...

--- Page 1 ---
Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed,
Diversity-Driven Policy Optimization
Tanmay Ambadkar1 Sourav Panda 2 Shreyash Kale 2 Jonathan Dodge 2 Abhinav Verma1
Abstract
Multi-objective reinforcement learning (MORL)
seeks to learn policies that balance multiple, of-
ten conflicting objectives. Although a single
preference-conditioned policy is the most flex-
ible and scalable solution, existing approaches
remain brittle in practice, frequently failing to re-
cover complete Pareto fronts. We show that this
failure stems from two structural issues in cur-
rent methods: destructive gradient interference
caused by premature scalarization and represen-
tational collapse across the preference space. We
introduce D3PO, a PPO-based framework that re-
organizes multi-objective policy optimization to
address these issues directly. D3PO preserves per-
objective learning signals through a decomposed
optimization pipeline and integrates preferences
only after stabilization, enabling reliable credit
assignment. In addition, a scaled diversity regu-
larizer enforces sensitivity of policy behavior to
preference changes, preventing collapse. Across
standard MORL benchmarks, including high-
dimensional and many-objective control tasks,
D3PO consistently discovers broader and higher-
quality Pareto fronts than prior single- and multi-
policy methods, matching or exceeding state-of-
the-art hypervolume and expected utility while
using a single deployable policy.
1. Introduction
Reinforcement learning (RL) has emerged as a powerful
framework for training agents to make sequential decisions
in complex environments. In the standard single-objective
setting (SORL), an agent interacts with an environment to
maximize the expected cumulative return of asingle scalar
1Department of Electrical Engineering and Computer Science,
Pennsylvania State University, PA, USA2College of Information
Sciences and Technology, Pennsylvania State University, PA, USA.
Correspondence to: Tanmay Ambadkar <tsa5252@psu.edu>, Ab-
hinav Verma<verma@psu.edu>.
Preprint. 2026-02-10.
reward function, which encodes the task’s objective (Sutton
& Barto, 1998). This paradigm has achieved remarkable
success in domains ranging from robotics and game playing
to recommendation systems and industrial control.
However, many real-world applications do not have a single
objective. Instead, they require agents to simultaneously
optimize multiple objectives that may besynergistic, con-
flicting, or context-dependent. For example, an autonomous
vehicle must trade off between speed, safety, fuel efficiency,
and passenger comfort. A logistics agent may need to bal-
ance delivery speed against cost and environmental impact.
In such scenarios, optimizing a single reward function col-
lapses the richness of the task, often leading to subopti-
mal or unsafe behaviors. This motivates the field ofMulti-
Objective Reinforcement Learning (MORL).
MORL extends the RL paradigm by decomposing all objec-
tives with avector of reward signals, where each element of
the vector corresponds to a different objective. It is possi-
ble that objectives conflict, such that improving the reward
in one objective reduces the reward in another. A single
policy cannot capture a global optimum (all objectives are
maximized). Instead of learning a single optimal policy, the
goal is to learn a set of Pareto-optimal policies. A policy
is Pareto-optimal if no other policy exists that can improve
at least one objective without worsening any other objec-
tive (Felten et al., 2024). Users can then select policies that
align with their preferences throughweight vectorsover the
objectives (Rodriguez-Soto et al., 2024). This setup enables
preference-driven decision makingand provides flexibility
for downstream deployment (Agarwal et al., 2022).
MORL introducesfundamental algorithmic and representa-
tional challengesthat go beyond those in SORL. A major
difficulty lies in thenon-uniqueness of optimal solutions:
the agent must learn to act optimally under multiple, often
contradictory reward structures. This requires reasoning
about trade-offs and responding to a potentially infinite set
of preference queries (Felten et al., 2024). When objectives
conflict, gradients derived from different reward signals may
point in opposing directions,destabilizing policy updates
and impairing sample efficiency(Liu et al., 2025a).
To cope with these challenges, existing MORL approaches
1
arXiv:2602.07764v1  [cs.LG]  8 Feb 2026
--- Page 2 ---
Decomposed, Diversity-Driven Policy Optimization
Agent
Environment Policy
Action(a)
Reward
(r1, r2,...,rn)
State(s)
Preference
Vector (w1, 
w2,...,wn)
obj1
obj2
.
.
.
.
objn
ℓ1
ℓ2
.
.
.
.
ℓn
A1
A2
An
Multi-Head
Critic
PPO 
Surrogate 
Losses
w2
w1
wn
Late Stage
Weighting
Diversity
Regularizer
Hopper2/3d
Ant2/3d
Humanoid
Actor Update
Minecart
Lunar Lander
Building9d
Critic Loss
Figure 1.Overview of the D3P Oframework. The architecture decouples credit assignment from preference integration to prevent
gradient interference.(1) Multi-Head Critic:The critic estimates independent per-objective values V (i)(s, ω)to compute unweighted
advantages A(i).(2) PPO Surrogate Losses:The clipping mechanism is applied to each advantage streamindependentlyEq. 2, stabilizing
the learning signal before scalarization.(3) Late-Stage Weighting:Preference weights ω are applied only to the stabilized surrogate
losses Eq. 4, ensuring gradients are not cancelled prior to optimization.(4) Diversity Regularizer:A diversity loss Eq. 3 is added to
force behavioral separation between different preference queries, preventing mode collapse.
have introduced various strategies. However, many con-
temporary methods face persistent limitations that hinder
their performance and scalability. First, methods that learn
a single policy often suffer fromdestructive gradient inter-
ference: naively combining conflicting objectives into one
learning signal produces opposing gradients, so an update
that improves one objective can harm another, leading to
training instability and suboptimal trade-off policies (Liu
et al., 2025a). Second, preference-conditioned policies fre-
quently exhibitincomplete front coveragethrough mode
collapse, where the network learns to produce only a small
set of similar behaviors for a wide range of preferences, leav-
ing large portions of the Pareto front unexplored. Finally,
multi-policy approaches that train a collection of separate
policies to cover the front suffer fromarchitectural inef-
ficiency, scaling poorly with the number of objectives and
incurring significant training and memory costs that make
them impractical for complex problems.
We present D3PO, illustrated in Figure 1, a framework for
training a single preference-conditioned policy for multi-
objective reinforcement learning that is stable and scalable
across diverse trade-offs. Our core contributions are:
• A practical formulation of single-policy MORL.We
show that single preference-conditioned policies can
reliably approximate high-quality Pareto fronts when
optimization is structured appropriately. Our approach
demonstrates that many failures of prior single-policy
methods arise from optimization design rather than
fundamental limitations of the paradigm.
• Decomposed optimization with late-stage prefer-
ence integration.We introduce a PPO-based learning
pipeline that preserves per-objective learning signals
and applies preference weights only after stabilization.
This design mitigates destructive gradient interference
caused by premature scalarization and enables stable
credit assignment across conflicting objectives.
• Scaled diversity regularization for preference sensi-
tivity.We propose a diversity regularizer that enforces
proportionality between preference differences and pol-
icy behavior differences. This mechanism prevents rep-
resentational collapse and ensures distinct preferences
map to distinct behaviors within a single policy.
• Competitive performance with deployment-efficient
single-policy learning.We show that a single
preference-conditioned policy can match or outper-
form multi-policy MORL baselines across benchmark
environments, including many-objective continuous
control. In addition to strong Pareto front quality, our
approach substantially reduces deployment complexity,
requiring orders of magnitude less memory than multi-
policy methods and eliminating the need for routing or
policy selection mechanisms.
2. Related Work
Multi-objective reinforcement learning (MORL) has devel-
oped along several algorithmic paradigms, each with distinct
strengths and limitations.
Scalarization.A foundational approach is scalarization,
which reduces vector rewards to a scalar for standard RL
methods. Linear scalarization (e.g., weighted sums) is com-
putationally efficient but limited to the convex regions of
2
--- Page 3 ---
Decomposed, Diversity-Driven Policy Optimization
the Pareto front. Nonlinear scalarization functions (Agarwal
et al., 2022; Rodriguez-Soto et al., 2024; Peng et al., 2025)
extend expressivity but still collapse objectives into a single
training signal, risking loss of information and instability
when objectives conflict.
Multi-policy methods.Other work trains a set of special-
ized policies for different preferences, then approximates the
Pareto front directly (Cai et al., 2023; Liu et al., 2025c; Hu
& Luo, 2024; Yang et al., 2025). Such approaches often rely
on constrained optimization or advanced multi-objective
optimization techniques to achieve high-quality fronts, but
scale poorly with the number of objectives due to the cost
of maintaining many policies.
Decomposition Based Approaches.Reward- and value-
decomposition methods form an influential class of ap-
proaches in multi-objective reinforcement learning. These
methods explicitly learn objective-specific value functions
or successor features and recombine them, typically through
generalized policy improvement (GPI), to derive policies
for different scalarizations without retraining (Barreto et al.,
2016; 2019). Variants based on linear scalarization similarly
maintain separate per-objective Q-functions and construct
policies by applying improvement operators over decom-
posed value components (Van Moffaert & Now ´e, 2014).
More recent work has enhanced GPI-based schemes by
prioritizing which decomposed components to update in
order to improve sample efficiency (Alegre et al., 2023).
While such approaches can be effective, they typically rely
on linear recombination assumptions and require maintain-
ing multiple value components or policies, and can incur
significant storage/compute overhead and limited smooth
interpolation across the middle of the Pareto front.
Single universal policies.To avoid training multiple poli-
cies, recent methods learn a single policy conditioned on
a preference vector, enabling adaptation at runtime (Yang
et al., 2019; Reymond et al., 2022; Basaklar et al., 2023;
Liu et al., 2025a; Kanazawa & Gupta, 2023). Examples
include Pareto-Conditioned Networks (PCN) (Reymond
et al., 2022), which reuse past transitions across prefer-
ences for sample efficiency; Preference-Driven MORL (PD-
MORL) (Basaklar et al., 2023), which combines preference
conditioning with off-policy engineering such as replay and
HER to scale to continuous control; and latent-conditioned
policy gradients (Kanazawa & Gupta, 2023), which em-
bed preferences in a latent space. Other PPO-style ex-
plorations (e.g., MOPPO (Terekhov & Gulcehre, 2024))
study empirical design choices for conditioned PPO vari-
ants. These methods demonstrate the practicality of univer-
sal preference-conditioned agents but suffer from gradient
interference or representational collapse.
Our contribution.D3PO belongs to this fourth family but
differs in two key respects, represented by the orange boxes
in Figure 1 : (i) it is anon-policyPPO extension with a
multi-head critic that preserves raw per-objective signals
and applies preferences only after PPO stabilization (Late-
Stage Weighting), and (ii) it introduces ascaled diversity
regularizer that provides a mechanism against mode col-
lapse. This combination of decomposed advantage preser-
vation, principled preference integration, and provable di-
versity offers a theoretically enriched alternative to prior
preference-conditioned methods, which have primarily em-
phasized empirical or off-policy approaches.
3. Preliminaries
We model decision-making problems with multiple ob-
jectives using aMulti-Objective Markov Decision Pro-
cess(MOMDP), formalized as the tuple: M=
⟨S,A, P, R1:d,Ω, γ⟩,where S is the state space,A is the ac-
tion space, P(s′ |s, a)is the transition probability function,
Ri(s, a)for i= 1, . . . , dare d objective-specific reward
functions, Ω :={ω∈R d
≥0|Pd
i=1 ωi = 1} denotes the
space of preference weights, and γ∈[0,1) is the discount
factor.
At each timestep t, the agent observes state st, chooses
an action at, and receives a reward vector rt =
[R1(st, at), . . . , Rd(st, at)]⊤ ∈R d. Given a prefer-
ence vector ω∈Ω , the overall goal is to find a pol-
icy πw that maximizes the expected scalarized return:
Eπ
P∞
t=0 γt ·ω ⊤rt

. The unweighted vector return corre-
sponding to a policy π is given by: Gπ :=E π [P∞
t=0 γtrt].
Pareto Optimality.Since no single policy can be optimal
for all preferences simultaneously, the goal of MORL is
to approximate thePareto front—a set of non-dominated
policies.
Definition 3.1(Pareto Dominance).Let u, v∈Rd be two
cumulative return vectors. Then u dominates v (denoted
u≻v ) if ui ≥v i for all i, and there exists at least one
objectivejsuch thatu j > vj.
Definition 3.2(Pareto-Optimal Policy).A policy π with a
return vector Gπ ∈R d isPareto-optimalif there is no other
policyπ ′ such thatG π′
dominatesG π.
To evaluate MORL algorithms, we use key metrics that quan-
tify both the quality and diversity of the learned Pareto front.
Hypervolume (HV)measures the volume of the objective
space dominated by the discovered front, encouraging both
Pareto-dominance and spread.Sparsity (SP)measures the
evenness of the discovered solutions along the front, with
lower values indicating better coverage.Expected Utility
(EU)measures the average performance across a distribu-
tion of sampled preference weights. Together, these metrics
assess both the fidelity (HV , EU) and diversity (SP) of the
learned solutions.
3
--- Page 4 ---
Decomposed, Diversity-Driven Policy Optimization
4. Method
We proposeDecomposed, Diversity Driven Policy Opti-
mization (D3PO), an extension of the standard PPO frame-
work designed to learn a single, unified policy that operates
effectively across a continuous spectrum of user-specified
preferences. While prior works have explored preference-
conditioned policies, they often rely on scalarizing the multi-
objective problem prematurely, leading to information loss
and challenges with gradient interference between com-
peting objectives. D 3PO addresses these limitations by
introducing a per-objective optimization framework that
maintains the vectorial nature of rewards and advantages
throughout the learning process. It promotes the actor to
learn different policies for different preferences by intro-
ducing a novel diversity driven loss function. This ap-
proach enables more stable training and produces a network
capable of working with any preference on the simplex
ω∈R d s.t. Pω= 1, ω≥0.
As illustrated in Figure 1, the D3P Oframework operates
via a decomposed optimization pipeline designed to prevent
gradient interference. The process begins with aMulti-
Head Criticthat estimates independent value functions for
each objective, which are used to compute per-objective
Generalized Advantage Estimations (GAE). These raw ad-
vantage signals are processed individually throughPer-
Objective PPO Surrogatesto ensure stability before being
aggregated viaLate-Stage Weightingusing the user’s pref-
erence vector ω. Finally, aDiversity Regularizeris added
to the actor loss to enforce that distinct preference queries
map to distinct behavioral modes.
4.1. Innovations
The core of D3PO lies in two innovations that adapt PPO
for the multi-objective setting. A detailed summary of the
complete method is available in Algorithm 1, found in Ap-
pendix A, alongside all Lemmas and Propositions.
Decomposed Policy Optimization with Dynamic Sam-
pling:We compute the PPO clipped surrogate objective
for each of the d advantages separately. We then derive the
final policy update by multiplying the preference weights
and clipped objectives. This ensures that PPO’s clipping
mechanism operates on theraw advantage signals, and the
weights ω are applied only after stabilization. As shown
in Proposition E.2, thisLate-Stage Weighting (LSW)pre-
serves the full information content of each advantage stream,
and avoids both the destructive cancellation of Early Scalar-
ization (ES) and the premature dampening of Mid-stage
Vectorial Scalarization (MVS). This can be visualized in
Figure 1 as the PPO Surrogate Losses which gets multiplied
with the objective weights to construct the final loss.
Scaled Diversity Regularization:To prevent mode col-
lapse, we introduce a loss term that increases the policy’s
behavioral diversity. This works by encouraging the KL di-
vergence between action distributions to be proportional to
the distance between their conditioning preferences. Propo-
sition F.2 proves that any minimizer of the resulting actor
objectivecannot exhibit representational mode collapse,
ensuring that distinct preferences map to distinct behav-
iors. This can be visualized in Figure 1 as the Diversity
Regularizer, which gets added to the Late Stage Weighting
constructed in the prior step.
4.2. Per-Objective Advantage and Value Estimation
Following trajectory collection, we compute the General-
ized Advantage Estimate (GAE) for each of the d objective
dimensions independently, yielding a d-dimensional advan-
tage vector At. The critic network, Vϕ(s, ω), approximates
the true state-value vector and is central to this process.
The critic utilizes a multi-head architecture (Figure 1 Green),
where a shared network body processes the state s and the
preference ω, feeding into d separate output heads. Each
head V (i)
ϕ is responsible for predicting theunweighted
valueof a single objective i. The critic is then updated
by minimizing the mean squared error between its predic-
tions and the empirical unweighted returnsG (i)
t :
Lcritic(ϕ) = 1
d
dX
i=1
Et

V (i)
ϕ (st, ω)−G(i)
t
2
(1)
Rationale for Conditioning on Preferences.A key design
choice is conditioning the critic Vϕ(s, ω)on the preference
vector ω even though it predicts unweighted returns. The
critic’s role is to estimate the state-value function V (i)
πω (s),
which is the expected unweighted return for objective i
when following the preference-conditioned policy π(·|s, ω).
Since the policy itself is a function of ω, the trajectories
it generates and the expected future returns are naturally
dependent on ω. Therefore, the critic must be conditioned
onωto accurately predict these policy-dependent values.
4.3. Policy Optimization with Decomposed Gradients
and Diversity Regularization
We update the actor network, πθ(a|s, ω)(Figure 1 Green),
over K epochs for each batch. Our policy optimization com-
bines the standard PPO objective, decomposed per-objective,
with a novel diversity-promoting regularizer to enhance the
policy’s ability to generalize across the preference space.
Per-Objective Policy Loss:We first compute the standard
PPO clipped surrogate objective independently for each of
the d advantage estimates (Figure 1 PPO Surrogate Losses).
This isolates the learning signal for each objective before
4
--- Page 5 ---
Decomposed, Diversity-Driven Policy Optimization
preference application:
L(i)
clip(θ) =E t
h
min

ρt(θ)A(i)
t ,clip(ρ t(θ),1−ϵ,1 +ϵ)A (i)
t
i
(2)
where the probability ratio is ρt(θ) = πθ(at|st,ω)
πθold (at|st,ω) . As ar-
gued in our theoretical analysis, this formulation ensures
that PPO’s stabilization mechanism is applied to each un-
weighted advantage, avoiding the signal distortion that
plagues ES and MVS.
Diversity-Promoting Regularization:Preference-
conditioned policies do not always map distinct preference
vectors ω to meaningfully distinct behaviors. To prevent
the policy from collapsing to similar strategies for different
preferences, we introduce an explicit diversity-promoting
loss. During each update, for a given preference ω, we
sample a “distractor” preference ω′ by adding small
Gaussian noise and re-projecting it onto the preference
simplex. (Figure 1 Diversity Regularizer)
We then define a diversity loss that penalizes the policy if
the distance between its action distributions, πθ(· |st, ω)
and πθ(· |st, ω′), does not match the distance between the
preferences themselves. We scale the target KL divergence
by the L1 distance between the preference vectors:
Ldiversity(θ) =E t
h 
DKL(πθ(· |st, ω)∥πθ(· |st, ω′))
−α∥ω−ω ′∥1
2i (3)
Proposition F.2 shows that minimizing this loss enforces a
proportionality between policy divergence and preference
divergence, thereby ruling out mode collapse and guarantee-
ing behavioral diversity.
Final Actor Objective:The actor’s objective combines two
distinct learning signals: (1) a policy improvement term
based on the PPO surrogate objective, and (2) our proposed
diversity regularizer. To update policy parameters θ, we
perform gradient descent on the combined loss function:
Lactor(θ) =−
 dX
i=1
ωiL(i)
clip(θ)
!
+λ divLdiversity(θ).(4)
Multiplying by the preference weight ωi is the critical step
translating the user’s desired trade-off into a concrete learn-
ing signal. Each L(i)
clip(θ) represents the raw PPO objective
for a single dimension. By scaling each term by its corre-
sponding weight ωi, we ensure that the final gradient is a
weighted sum of the per-objective gradients. This steers
the policy update in a direction that prioritizes improving
higher weighted objectives, while retaining stability and
information preservation guaranteed by Lemma E.1 and
Proposition E.2. The term λdiv controls the strength of the
diversity regularization, which by Proposition F.2 guaran-
tees preference-dependent behavioral separation.
5. Analysis of the D3PO Framework
The success of D3PO arises not from a single algorithmic
trick, but from a synergistic framework designed to resolve
two fundamental challenges in training a single preference-
conditioned policy: (1) achievingstable credit assignment
in the presence of conflicting objectives, and (2) ensuring
the learned policygeneralizes across the preference man-
ifoldrather than collapsing to a limited set of behaviors.
Our framework addresses these challenges through three
complementary innovations: decomposed value estimation,
principled late-stage preference integration, and scaled di-
versity regularization. Each design choice is motivated by
intuition and supported by theoretical analysis, with proofs
in the Appendix.
Stable Credit Assignment via Decomposition:The first
principle of D3PO isdecomposed optimization, beginning
with the critic. The multi-head critic predicts the unweighted
expected return V (i)(s, ω)for each objective i, and GAEs
are computed independently, yielding a d-dimensional ad-
vantage vector At. This preserves a distinct, interference-
free credit signal for each objective.
Intuitively, this avoids contaminating the learning sig-
nal with preference-based mixtures too early. Formally,
Lemma E.1 shows that scalarizing advantages before opti-
mization (as in Early Scalarization, ES) inevitably discards
information: the magnitude of the scalarized advantage|Aω
t |
is strictly smaller than the sum of individual magnitudes
whenever objectives conflict. This phenomenon, which we
termadvantage cancellation, explains why ES-based meth-
ods (e.g., MOPPO (Terekhov & Gulcehre, 2024)) often stall
under conflicting objectives.
Principled Preference Integration via Late-Stage Weight-
ing:While decomposition preserves raw signals, preference
weighting must still be integrated in a way that avoids dis-
tortion. Traditional methods either weight too early (ES) or
dampen signals before PPO stabilization (Mid-stage Vecto-
rial Scalarization, MVS). Both approaches risk destructive
interference or overly conservative updates.
D3PO instead employsLate-Stage Weighting (LSW): PPO
surrogates are computed on raw per-objective advantages,
and only the stabilized losses are weighted by preferences.
This design decouples PPO’s trust region stabilization from
user preference scaling: the stabilization mechanism oper-
ates on true credit signals, and preferences act only as a final
arbitration.
Intuitively, this ensures that PPO “sees” the full significance
of each event before preferences adjust its contribution. For-
mally, Proposition E.2 shows that LSW preserves advantage
magnitudes while MVS and ES distort them, establishing
the robustness hierarchy
5
--- Page 6 ---
Decomposed, Diversity-Driven Policy Optimization
LSW⪰MVS≻ES.
This hierarchy guarantees that D 3PO avoids gradient in-
terference and remains sensitive to high-magnitude events,
even for objectives with low weights. The full proof is in
Appendix E. The proof gives a precise mathematical ba-
sis for the design choice of LSW: When pipelines include
per-objective normalization, per-objective ratios, adaptive
clipping, or other non-homogeneous operators (common in
practice), LSW preserves stabilized event magnitudes better
than MVS (Proposition E.4).
Preventing Collapse via Diversity Regularization:Stable
credit assignment alone is not sufficient. A common failure
mode of preference-conditioned agents ismode collapse, or
“policy laziness,” where the policy produces nearly identical
behaviors across wide regions of the preference simplex.
This limits the ability to recover the full Pareto front.
D3PO counters this with a scaled diversity regularizer. In-
tuitively, this regularizer ensures sensitivity to preferences
and prevents collapse to a singleaveragepolicy. Formally,
Proposition F.2 proves that any minimizer of the combined
actor objective cannot exhibit mode collapse: distinct pref-
erences must yield distinguishable action distributions. This
is the first formal guarantee of anti-collapse in preference-
conditioned MORL. This guarantees D3PO is stable in prac-
tice and sound across tabular and neural regimes.
Synergy and Broader Context:The strength of D 3PO lies
in the synergy of these components:Decomposed value
estimationprovides clean, per-objective signals;Late-Stage
Weightingintegrates preferences without interference;Di-
versity regularizationensures generalization and prevents
collapse and catastrophic forgetting, which is a problem
single-policy techniques suffer. Together, these components
yield a framework that is more robust to advantage cancella-
tion, less prone to collapse, and convergent under standard
conditions. Compared to MOPPO, which suffers from ES’s
cancellation (Lemma E.1), and Pareto-Conditioned Net-
works, which lack collapse guarantees, D 3PO introduces
a preference-conditioned PPO approach with theoretical
support for both stability and diversity.
Occam’s Razor in MORL.The contrast between D 3PO
and existing methods illustrates a broader principle:when
sophisticated baselines fail, the solution often lies in identi-
fying core pathologies rather than adding complexity. Multi-
policy methods (C-MORL, PG-MORL) maintain hundreds
of separate networks and require routing or interpolation
among a discrete set of trained policies. This becomes brit-
tle when user preferences fall between or outside the trained
points, and scales poorly with the number of objectives.
Decomposition methods (GPI-LS) require maintaining mul-
tiple value components with careful prioritization schemes.
In contrast, D3PO directly addresses the two fundamental
issues with minimal modifications to vanilla PPO, map-
ping any continuous preference vector ω to a valid behavior
through π(a|s, ω) without routing, interpolation, or policy
ensembles. This ensures smooth, predictable adaptation
across the entire preference space while using orders of
magnitude fewer parameters (Table 9).
6. Experiments
We evaluate our proposed method,D3PO, against state-of-
the-art baselines to answer three key questions: (1) Does
D3PO achieve comprehensive Pareto front coverage? (2)
Does it effectively prevent mode collapse and generate di-
verse solutions? (3) Is it computationally efficient?
Our evaluation uses a suite of challenging MORL tasks
from the MO-Gymnasium library (Felten et al., 2023), in-
cluding five continuous control and two discrete control
environments, and additionally the Building-9d environ-
ment, introduced in (Liu et al., 2025b). We compare D3PO
against five strong baselines:PCN(Reymond et al., 2022),
GPI-LS(Alegre et al., 2023),C-MORL(Liu et al., 2025b),
PG-MORL(Xu et al., 2020), andCAPQL(Lu et al., 2023).
For discrete tasks, the number of environment interactions
was 5×10 5 steps. For the more complex continuous control
environments, we scaled the number of environment inter-
actions with the number of objectives: 1.5×10 6, 2×10 6,
and 2.5×10 6 steps for tasks with two, three, and nine ob-
jectives, respectively. We have used the same number of en-
vironment interactions as C-MORL (Liu et al., 2025b). We
measured performance with Hypervolume (HV), Expected
Utility (EU), Sparsity (SP), and total training Compute Time
(CT). Further experimental details are in the appendix.
D3PO Improves Pareto Front Coverage.The results in
Table 1, 2 and Figure 2 show that D 3PO finds dominant
and complete solution sets. Quantitatively, D3PO competi-
tively performs (achieves statistically significant improve-
ments - Table) in both Hypervolume and Expected Utility.
The significance experiments are analysed in Appendix I.2.
In the highly complex MO-Humanoid-2d task, D 3PO ob-
tains the highest HV and EU. The advantage is even more
pronounced in the nine-objective Building-9d environment,
where some baselines (PG-MORL, GPI-LS) failed to com-
plete training within the time limit (5 days). In contrast,
D3PO not only finished but also achieved the best metrics.
Visually, the Pareto fronts in Figure 2 show D3PO (red) dis-
covering solutions that envelop the baselines. In MO-Ant-
2d, for instance, D3PO identifies high-performance “special-
ist” policies at the extremes of the trade-off space that other
methods miss. This superior coverage stems from our core
methodological contributions. By computing a vectorized,
per-objective advantage and using decomposed policy gra-
dients, D3PO mitigates the destructive gradient interference
6
--- Page 7 ---
Decomposed, Diversity-Driven Policy Optimization
Table 1.Performance comparison oncontinuousenvironments (Hopper, Ant, Humanoid, Building-9d). Metrics: Hypervolume (HV),
Expected Utility (EU), Sparsity (SP), and Compute Time (CT).T/Oindicates timeout after 5 days.
Environment Metrics CAPQL PG-MORL GPI-LS C-MORL D 3PO
Hopper-2d
HV (105 ↑)1.15±0.08 1.20±0.09 1.19±0.101.37±0.031.30±0.03
EU (102 ↑)2.28±0.07 2.34±0.10 2.33±0.102.53±0.022.47±0.01
SP (102 ↓)0.46±0.10 5.13±5.81 0.49±0.37 1.13±0.190.26±0.31
CT (↓) 3 hours 8 hours 12 hours 36 mins20 mins
Hopper-3d
HV (107 ↑)1.65±0.45 1.59±0.45 1.70±0.292.19±0.322.12±0.16
EU (102 ↑)1.53±0.28 1.47±0.25 1.62±0.101.81±0.011.74±4.9
SP (102 ↓)2.31±3.16 0.76±0.91 0.74±1.22 0.53±0.340.04±0.01
CT (↓) 2 hours 6 hours 15 hours 48 mins30 mins
Ant-2d
HV (105 ↑)1.11±0.69 0.35±0.08 1.17±0.25 1.31±0.161.91±0.18
EU (102 ↑)2.16±0.94 0.81±0.23 4.28±0.19 2.50±0.253.14±0.21
SP (103 ↓)0.18±0.072.20±3.48 3.61±2.13 2.65±1.25 0.66±0.40
CT (↓) 5 hours 8 hours 11 hours 78 mins35 mins
Ant-3d
HV (107 ↑)1.22±0.33 0.94±0.12 0.55±0.81 2.61±0.262.68±0.21
EU (102 ↑)1.30±0.29 1.07±0.07 2.41±0.202.06±0.141.99±0.08
SP (103 ↓)0.17±0.09 0.02±0.01 1.96±0.79 0.06±0.070.004±0.002
CT (↓) 3 hours 10 hours 19 hours 66 mins45 mins
Humanoid-2d
HV (105 ↑)3.30±0.06 2.62±0.32 1.98±0.02 3.43±0.063.76±0.11
EU (102 ↑)4.75±0.04 4.06±0.32 3.67±0.02 4.78±0.055.11±0.09
SP (104 ↓)0 ∗ 0.13±0.17 0 ∗ 2.21±3.470.003±0.001
CT (↓) 3 hours 7 hours 18 hours 55 mins30 mins
Building-9d
HV (1031 ↑)4.29±0.73T/O T/O7.93±0.078.00±0.11
EU (103 ↑)3.31±0.06T/O T/O3.50±0.003.50±0.003
SP (103 ↓)4.34±3.72T/O T/O2.79±0.400.03±0.01
CT (↓) 15 hoursT/O T/O55 mins45 mins
common in MORL. This process preserves a clean credit
assignment signal for each objective, boosting the policy’s
ability to better exploit the reward landscape and master a
wider range of trade-offs.
Diversity Regularization Prevents Mode Collapse.A
common failure in preference-conditioned MORL is mode
collapse, where the policy produces only a single behavior
for all preferences. Our second research question investi-
gates how D3PO avoids this.
The most direct evidence is in the MO-Humanoid-2d results
(Table 1), where several baselines report a Sparsity (SP)
of 0. This indicates a total collapse to a single dominant
policy. In contrast, D3PO achieves a low but non-zero SP
(0.003×104), demonstrating that it has learned a diverse and
well-distributed set of policies across the front. The visual
results in Figure 2 further confirm that D3PO discovers rich,
well-spaced pareto fronts.
Diverse policies are primarily due to our proposed scaled
diversity regularization. As shown in our ablation study
(Table 4), removing the diversity loss (D3PO-DDPO) results
in a clear performance drop and, in some cases, collapse to
a single-point front (e.g., Humanoid-2d). This highlights
that explicitly encouraging the policy to produce distinct
behaviors for distinct preferences is critical for discovering
a complete and useful Pareto front.
D3PO Offers Better Computational Efficiency.Finally,
we address the question of efficiency. D3PO is significantly
faster than many competing methods because it avoids com-
mon computational bottlenecks. Table 1 and 2 shows the
total training wall clock time required to train all baselines
and D3PO. We can see that D3PO provides a good speedup
when compared to the baselines.
Unlike evolutionary or archive-based methods like PG-
MORL and CMORL, D3PO does not require an expensive
select-and-improveloop which selects a solution from a pop-
ulation for further training. Instead, its training process is a
continuous, end-to-end optimization analogous to standard
PPO, which saves considerable compute time by learning
the entire policy manifold simultaneously.
While D3PO consistently achieves competitive results across
most benchmarks, we note that C-MORL outperforms on
Hopper-2d and Hopper-3d in terms of HV and EU (Table 1).
This difference arises from the inherent methodological con-
trast: C-MORL focuses on iteratively improving existing
Pareto solutions, which allows it to refine certain extreme
trade-offs and expand the hypervolume. In contrast, D3PO
discovers a uniform Pareto front that captures the majority
of the trade-off surface but does not fully cover the extremes.
As a result, C-MORL attains slightly better HV and EU at
the cost of higher sparsity, whereas D3PO maintains lower
sparsity and competitive overall coverage. C-MORL’s appar-
7
--- Page 8 ---
Decomposed, Diversity-Driven Policy Optimization
Figure 2.Pareto front comparison on two-objective MO-MuJoCo benchmarks. D 3PO (red) discovers a uniform and well-distributed front
across the trade-off space, whereas C-MORL (blue) refines extreme points at the cost of higher sparsity. Compared to CAPQL, GPI-LS,
and PG-MORL, D3PO achieves broader coverage and reduced collapse, particularly visible in Ant and Humanoid.
ent performance differences are not statistically significant
(Appendix I.2), indicating that it does not achieve a mean-
ingful advantage over D3PO.
Ablations.We introduced two modifications to the actor
loss function that allow for the discovery of diverse, evenly
spaced Pareto fronts previously inaccessible to single-policy
MORL. We conducted ablation experiments to understand
the impact of our changes - Late Stage Weighting(LSW)
and Diversity-driven policy optimization(DDPO). First, we
removeLSWby multiplying the preference weights with
the advantages after rollout collection, thereby collecting the
weighted advantages instead of the unweighted advantages
(in effect, ES). In this experiment, we do not remove the
diversity loss. Second, we turn off the diversity loss and
keep the original decomposed gradient function.
Table 4 shows that both additions are necessary for D3PO’s
success. Turning off LSW (column 2), makes the perfor-
mance suffer considerably. This shows that learning accu-
rate unweighted returns is necessary to drive correct gradient
updates. When we turn onLSWand turn offDDPO(col-
umn 3), we see that the performance improves significantly
but it still does not fully approximate the whole front. In
both cases, the policies converged prematurely to a single
point front in the Humanoid environment. For Hopper and
Ant the combination of low HV , EU and SP values shows
that they discovered an inferior Pareto front compared to
D3PO. These experiments show that both innovations are
necessary to learn robust policies that approximate a high
quality Pareto Front in the single-policy MORL setting.
Further, Appendix D presents an ablation over the loss scal-
ing parameter λdiv, showing that while the diversity regular-
izer itself is essential, the discovered front is robust to the
precise value of λdiv. An ablation on the α parameter also
shows that the scaling parameter does not affect the results,
unless it is explicitly turned off, which results in collapse,
or set to a very high value, which diminishes the KL term.
Limitations.While D 3PO demonstrates strong performance
on the evaluated benchmarks, it is not universally applica-
ble to all MORL settings. A key limitation arises from
the diversity regularizer, which explicitly encourages dis-
tinct behaviors for distinct preference vectors. This assump-
tion may not hold in environments with highly discrete or
piecewise-constant Pareto fronts, such as FruitTree-style do-
mains, where multiple preference weights can correspond to
the same optimal policy. In such cases, enforcing behavioral
separation may be unnecessary or even counterproductive.
In contrast, many continuous control problems exhibit
smooth Pareto fronts, where changes in preference natu-
rally induce changes in optimal behavior. In these settings,
the diversity regularizer aligns well with the underlying
structure of the problem and helps prevent policy collapse,
enabling a single preference-conditioned policy to recover a
broad and well-distributed set of trade-offs. The empirical
results suggest that D3PO is particularly well suited to such
continuous, real-world domains.
7. Conclusion
In this work, we introduced D 3PO, a novel algorithm for
training a single, generalizable policy for MORL. We iden-
tified two critical challenges that hinder prior preference-
conditioned methods: destructive gradient interference and
representational mode collapse. Our proposed framework
addresses these issues through a synergy of two principled
mechanisms: a decomposed optimization process that pre-
serves the integrity of per-objective credit assignment, and
a scaled diversity regularization term that enforces a robust
and high-fidelity mapping from the preference space to the
policy manifold. Our experiments demonstrate that these
two targeted additions to PPO are necessary and sufficient
to achieve state-of-the-art MORL performance. D3PO dis-
covers more complete and higher-quality Pareto fronts than
existing methods, with particularly pronounced advantages
in complex, high-dimensional control and many-objective
scenarios.
8
--- Page 9 ---
Decomposed, Diversity-Driven Policy Optimization
Impact Statements
This paper presents work whose goal is to advance the field
of machine learning. There are many potential societal
consequences of our work, none of which we feel must be
specifically highlighted here.
References
Agarwal, M., Aggarwal, V ., and Lan, T. Multi-objective re-
inforcement learning with non-linear scalarization. In
Proceedings of the 21st International Conference on
Autonomous Agents and Multiagent Systems, AAMAS
’22, pp. 9–17. International Foundation for Autonomous
Agents and Multiagent Systems, 2022.
Alegre, L. N., Bazzan, A. L., Roijers, D. M., Now´e, A., and
da Silva, B. C. Sample-efficient multi-objective learning
via generalized policy improvement prioritization.arXiv
preprint arXiv:2301.07784, 2023.
Barreto, A., Dabney, W., Munos, R., Hunt, J., Schaul, T.,
van Hasselt, H., and Silver, D. Successor features for
transfer in reinforcement learning. arxiv.arXiv preprint
arXiv:1606.05312, 2016.
Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D.,
Hessel, M., and Mankowitz, D. A. ˇzıdek, and r.
munos,“transfer in deep reinforcement learning using
successor features and generalised policy improvement,”.
arXiv preprint arXiv:1901.10964, 2019.
Basaklar, T., Gumussoy, S., and Ogras, U. PD-MORL:
Preference-driven multi-objective reinforcement learning
algorithm. InThe Eleventh International Conference
on Learning Representations, 2023. URL https://
openreview.net/forum?id=zS9sRyaPFlJ.
Cai, X.-Q., Zhang, P., Zhao, L., Bian, J., Sugiyama, M.,
and Llorens, A. J. Distributional Pareto-Optimal multi-
objective reinforcement learning. InAdvances in Neural
Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=prIwYTU9PV.
Felten, F., Alegre, L. N., Now´e, A., Bazzan, A. L. C., Talbi,
E. G., Danoy, G., and Silva, B. C. da. A toolkit for reliable
benchmarking and research in multi-objective reinforce-
ment learning. InProceedings of the 37th Conference on
Neural Information Processing Systems (NeurIPS 2023),
2023.
Felten, F., Talbi, E.-G., and Danoy, G. Multi-objective
reinforcement learning based on decomposition: A tax-
onomy and framework.Journal of Artificial Intelli-
gence Research, 79:679–723, 2024. doi: 10.1613/jair.1.
15702. URL https://doi.org/10.1613/jair.
1.15702.
Hu, T. and Luo, B. PA2D-MORL: Pareto Ascent direc-
tional decomposition based multi-objective reinforce-
ment learning. InProceedings of the Thirty-Eighth
AAAI Conference on Artificial Intelligence. AAAI Press,
2024. doi: 10.1609/aaai.v38i11.29148. URL https:
//doi.org/10.1609/aaai.v38i11.29148.
Kanazawa, T. and Gupta, C.Latent-Conditioned Pol-
icy Gradient for Multi-Objective Deep Reinforcement
Learning, pp. 63–76. Springer Nature Switzer-
land, 2023. ISBN 9783031442230. doi: 10.1007/
978-3-031-44223-0 6. URL http://dx.doi.org/
10.1007/978-3-031-44223-0_6.
Liu, E., Wu, Y .-C., Huang, X., Gao, C., Wang, R.-J., Xue,
K., and Qian, C. Pareto set learning for multi-objective
reinforcement learning, 2025a. URL https://arxiv.
org/abs/2501.06773.
Liu, R., Pan, Y ., Xu, L., Song, L., You, P., Chen, Y ., and
Bian, J. Efficient discovery of pareto front for multi-
objective reinforcement learning. InThe Thirteenth
International Conference on Learning Representations,
2025b. URL https://openreview.net/forum?
id=fDGPIuCdGi.
Liu, R., Pan, Y ., Xu, L., Song, L., You, P., Chen, Y ., and
Bian, J. Efficient discovery of Pareto front for multi-
objective reinforcement learning. InThe Thirteenth
International Conference on Learning Representations,
2025c. URL https://openreview.net/forum?
id=fDGPIuCdGi.
Lu, H., Herman, D., and Yu, Y . Multi-objective reinforce-
ment learning: Convexity, stationarity and pareto opti-
mality. InThe Eleventh International Conference on
Learning Representations, 2023.
Peng, N., Tian, M., and Fain, B. Multi-objective rein-
forcement learning with nonlinear preferences: Provable
approximation for maximizing expected scalarized re-
turn, 2025. URL https://arxiv.org/abs/2311.
02544.
Reymond, M., Bargiacchi, E., and Now´e, A. Pareto condi-
tioned networks, 2022. URL https://arxiv.org/
abs/2204.05036.
Rodriguez-Soto, M., Rodriguez Aguilar, J. A., and L´opez-
S´anchez, M. An analytical study of utility functions in
multi-objective reinforcement learning. InThe Thirty-
eighth Annual Conference on Neural Information Pro-
cessing Systems, 2024. URL https://openreview.
net/forum?id=K3h2kZFz8h.
Sutton, R. S. and Barto, A. G.Reinforcement Learning: An
Introduction. The MIT Press, Cambridge, MA, 1998.
9
--- Page 10 ---
Decomposed, Diversity-Driven Policy Optimization
Terekhov, M. and Gulcehre, C. In search for
architectures and loss functions in multi-objective
reinforcement learning.ArXiv, abs/2407.16807,
2024. URL https://api.semanticscholar.
org/CorpusId:271404860.
Van Moffaert, K. and Now´e, A. Multi-objective reinforce-
ment learning using sets of pareto dominating policies.
The Journal of Machine Learning Research, 15(1):3483–
3512, 2014.
Xu, J., Tian, Y ., Ma, P., Rus, D., Sueda, S., and Matusik, W.
Prediction-guided multi-objective reinforcement learning
for continuous robot control. InInternational conference
on machine learning, pp. 10607–10616. PMLR, 2020.
Yang, R., Sun, X., and Narasimhan, K. A generalized
algorithm for multi-objective reinforcement learning and
policy adaptation, 2019. URL https://arxiv.org/
abs/1908.08342.
Yang, Y ., Zhou, T., Pechenizkiy, M., and Fang, M.
Preference controllable reinforcement learning with
advanced multi-objective optimization. InForty-
second International Conference on Machine Learning,
2025. URL https://openreview.net/forum?
id=49g4c8MWHy.
10
--- Page 11 ---
Decomposed, Diversity-Driven Policy Optimization
A. D3PO Pseudocode
Algorithm 1Decomposed, Diversity-Driven Policy Optimization
Require: Actor πθ(a|s, ω) , multi-head critic Vϕ(s, ω)∈R d, Optimizers Optθ,Opt ϕ, and hyperparameters
γ, λ, ϵ, β, λdiv, α
1:Initialize network parametersθ, ϕand rollout bufferD
2:Sample an initial preference vectorωfrom the preference spaceΩ
3:foriteration= 1,2, . . .do
4:Clear rollout bufferD
5:fort= 1toTdo
6:Sample actiona t ∼π θ(· |st, ω)
7:Executea t and observe next states t+1, reward vectorr t ∈R d, and done flagd t
8:Store transition(s t, at,r t, ω,logπθ(at |s t, ω))inD
9:s t ←s t+1
10:ifd t is Truethen
11:Reset environment to get new states t and resample a new preference vectorω∼Ω
12:end if
13:end for
14:Compute unweighted advantagesA t = [A(1)
t , . . . , A(d)
t ]and returnsG t for all transitions inDusing GAE withV ϕ.
15:forepoch= 1toEdo
16:foreach minibatchB ⊂ Ddo
17:Let(s, a,A,G, ω,logπ old)be the data inB
18:Predict value vectorV ϕ(s, ω) = [V(1)
ϕ , . . . , V(d)
ϕ ]
19:L critic ← 1
d
Pd
i=1

V (i)
ϕ (s, ω)−G(i)
2
20:Update critic parametersϕusingOpt ϕ and∇ ϕLcritic
21:Sample distractor weightsω ′ by perturbing and re-normalizingω
22:Compute per-objective PPO losses{L (i)
clip}d
i=1 using unweighted advantagesA
23:Compute diversity lossL diversity(θ) =E s∈B
h 
DKL(πθ(· |s, ω)∥πθ(· |s, ω′))−α∥ω−ω ′∥1
2i
24:Compute entropy bonusH ←E s∈B[H(πθ(· |s, ω))]
25:L actor ← −
Pd
i=1 ωiL(i)
clip

−βH+λ divLdiversity
26:Update actor parametersθusingOpt θ and∇ θLactor
27:end for
28:end for
29:end for
B. Metrics Definitions
Definition B.1(Hypervolume Indicator).Given a reference point r∈R d that all Pareto-optimal returns dominate, the
hypervolumeof a finite set{u k}is, whereLMstands for Lebesgue Measure:
HV({uk};r) =LM
 [
k
{u∈R d :r≤u≤u k}
!
Definition B.2(Sparsity Indicator).Let {u1, . . . , uK} ⊂Rd be an ordered set of Pareto-approximated points. Define the
sparsityas:
SP({uk}) = 1
K−1
K−1X
k=1
∥u(k+1) −u (k)∥2
Definition B.3(Expected Utility).Let W ⊂Rd be a distribution over preference weights and let πω denote the policy
conditioned onω. Theexpected utilityis:
EU =E ω∼W[ω ⊤Gπω ].
11
--- Page 12 ---
Decomposed, Diversity-Driven Policy Optimization
Definition B.4(Compute Time).The compute time is defined as the time taken by the algorithm to complete its training
given the fixed budget of environment interactions. It is calculated as the wall clock time required to complete the entire
training pipeline
C. Discrete Environments Results
Table 2.Performance comparison ondiscreteenvironments (Minecart, Lunar Lander-4d). Metrics: Hypervolume (HV), Expected Utility
(EU), Sparsity (SP), and Compute Time (CT).
Environment Metrics PCN GPI-LS C-MORL D 3PO
Minecart
HV (102 ↑)5.32±4.28 6.05±0.37 6.77±0.887.39±0.08
EU (10−1 ↑)1.5±0.012.29±0.322.12±0.66 1.9±0.06
SP (10−1 ↓)0.1±0.01 0.10±0.00 0.05±0.020.01±0.01
CT (↓) 6 hours 5 hours 16 mins7 mins
Lunar Lander-4d
HV (109 ↑)0.78±0.17 1.06±0.16 1.12±0.031.23±0.04
EU (101 ↑)1.44±0.37 1.81±0.342.35±0.18 2.39±0.19
SP (103 ↓)0.03±0.230.13±0.01 1.04±0.24 0.32±0.16
CT (↓) 7 hours 5 hours 20 mins10 mins
Table 3.Performance comparison on the Fruit Tree environment.
Environment Metrics GPI-LS C-MORL D 3PO
Fruit Tree
HV (104 ↑)3.57±0.053.52±0.12 3.42±0.07
EU (↑)6.15±0.006.53±0.084.62±0.02
SP (↓)5.29±0.21 0.14±0.010.04±0.01
Table 3 presents the performance comparison on the Fruit Tree environment. The results highlight a significant distinction in
the optimization behaviors of the evaluated algorithms. WhileGPI-LSachieves the highest Hypervolume ( 3.57×10 4) and
C-MORLyields the highest Expected Utility ( 6.53),D 3POdemonstrates superior performance in solution quality and
diversity.
Most notably,D 3POachieves extremely low sparsity (700 points on the front). While D 3PO yields a slightly lower
Hypervolume (3.42×10 4) compared to the baselines, this metric trade-off suggests a fundamental difference in exploration
strategy:
• GPI-LSappears to maximize Hypervolume by identifying a few extreme, high-reward outliers, as evidenced by its
high sparsity score. This leaves large gaps in the objective space, limiting the decision-maker’s choices.
• D3PO, conversely, prioritizes a high-resolution coverage of the trade-off curve. By successfully recovering the dense
“middle” regions of the non-convex front, D3PO provides a smooth, continuous set of solutions.
C-MORL is not able to provide beyond 200 policies without hurting the performance. D3PO offers superior value for tasks
requiring granular control over objective trade-offs, ensuring that no region of the Pareto front is neglected in favor of
extreme points.
D. Ablation Experiments
Table 5.Ablation results on MO-Humanoid-2d across different values of λdiv. The results show that the discovered Pareto front remains
stable and high-performing over a wide range ofλ div, indicating robustness of the method to this hyperparameter.
Metricλ div = 0λ div = 0.01λ div = 0.1λ div = 0.5λ div = 1.0
HV (105 ↑)2.32±0.053.76±0.113.73±0.07 3.72±0.10 3.73±0.07
EU (102 ↑)3.83±0.055.11±0.095.08±0.06 5.07±0.09 5.07±0.06
SP (103 ↓)0 ∗ 0.03±0.010.047±0.045 0.059±0.044 0.053±0.032
12
--- Page 13 ---
Decomposed, Diversity-Driven Policy Optimization
Table 4.Ablation results showing the contributions of Late Stage Weighting (LSW) and Diversity-Driven Policy Optimization (DDPO) in
D3PO. LSW improves stability but often collapses the Pareto front (SP = 0), while DDPO preserves diversity and yields more uniform
fronts. The full D3PO consistently achieves the best trade-off across HV , EU, and SP.
Environment Metrics D 3POD 3PO\LSWD 3PO\DDPO
Humanoid-2d
HV (105 ↑)3.76±0.111.50±0.17 2.32±0.05
EU (102 ↑)5.11±0.092.87±0.22 3.83±0.05
SP (104 ↓)0.003±0.0010 ∗ 0∗
Hopper-2d
HV (105 ↑)1.30±0.031.23±0.03 1.22±0.06
EU (102 ↑)2.47±0.012.38±0.05 2.42±0.05
SP (102 ↓)0.26±0.31 0.08±0.020.04±0.02
Ant-2d
HV (105 ↑)1.91±0.181.53±0.11 1.86±0.07
EU (102 ↑)3.14±0.212.71±0.13 3.09±0.06
SP (103 ↓)0.66±0.400.18±0.070.36±0.09
Table 5 reports ablation results on Humanoid-2d across a sweep of λdiv values. These results demonstrate that the diversity
regularizer itself plays a critical role in shaping the discovered Pareto front. Without diversity encouragement (λdiv = 0), the
algorithm collapses toward limited modes, yielding weaker hypervolume and expected utility despite producing seemingly
low sparsity values. Introducing a nonzero regularizer ( λdiv >0 ) resolves this issue by preventing mode collapse and
maintaining broad front coverage, thereby producing substantially stronger Pareto sets.
At the same time, the quantitative metrics reveal that the performance is relatively insensitive to the precise choice ofλdiv.
Across the range λdiv ∈ {0.01,0.1,0.5,1.0}, hypervolume and expected utility remain consistently high, and sparsity values
remain comparable. This indicates that while the presence of the diversity term is essential, its specific scaling does not
heavily influence the outcome. Overall, these ablations reinforce that the diversity regularizer is the key mechanism enabling
robust front discovery, and that the method is not fragile to the exact tuning ofλdiv.
Metricα= 0α= 0.1α= 1α= 10
HV (105 ↑)2.50±0.12 3.71±0.083.76±0.113.20±0.10
EU (102 ↑)3.90±0.09 5.03±0.075.11±0.094.80±0.27
SP (103 ↓)0 ∗ 0.07±0.020.03±0.010.12±0.08
Table 6.Ablation results on MO-Humanoid-2d across different values ofα.
Table 6 reports similar results. When α= 0 , the weights scaling parameter is turned off. This keeps the KL term active, and
the loss function now tries to minimize the KL. By minimizing the KL, the function actively promotes collapse. Thus, α is
an extremely important parameter. When α= 0.1 and α= 1 , the results are similar. This shows that D3PO is robust to
the values of the weight parameter. Choosing a very high value α= 10 is also detrimental to performance, as that term
dominates the loss function. Thus, a reasonable choice forαis between 0.1 and 1.
E. Theoretical Analysis of Multi-Objective PPO Formulations
To justify the design of our proposed Late-Stage Weighting (LSW) framework, we provide a formal, unified comparative
analysis of three distinct methods for integrating preference weights into the Proximal Policy Optimization (PPO) objective.
We prove that LSW is the most robust formulation against the signal distortion caused by conflicting advantages and
preference scaling, and we characterize precisely when differences between MVS and LSW arise in practice.
E.1. Formal Definitions of MORL-PPO Variants
Let
ρt(θ) = πθ(at |s t, ω)
πθold (at |s t, ω)
13
--- Page 14 ---
Decomposed, Diversity-Driven Policy Optimization
be the importance sampling ratio and At = [A(1)
t , . . . , A(d)
t ] the vector of per-objective advantages. We compare three
natural ways to incorporate the preference vectorω∈∆ d−1 into a PPO-style surrogate.
Method 1: Early Scalarization (ES).Scalarize advantages first, then apply the PPO surrogate (Terekhov & Gulcehre,
2024):
LES
clip (θ) =E t
h
min
 
ρt(θ) (ω⊤At),clip(ρ t(θ),1−ϵ,1 +ϵ) (ω ⊤At)
i
.(5)
Method 2: Mid-stage Vectorial Scalarization (MVS).Form per-objective weighted advantages, apply per-objective
surrogates, then sum:
LMV S
actor (θ) =−
dX
i=1
Et
h
min
 
ρt(θ) (ωiA(i)
t ),clip(ρ t(θ),1−ϵ,1 +ϵ) (ω iA(i)
t )
i
.(6)
Method 3: Late-Stage Weighting (LSW).Compute per-objective PPO surrogates on raw advantages and weight the
resulting stable surrogate terms:
LLSW
actor (θ) =−
dX
i=1
ωi Et
h
min
 
ρt(θ)A (i)
t ,clip(ρ t(θ),1−ϵ,1 +ϵ)A (i)
t
i
.(7)
E.2. Comparative Results
We now formalize the intuition that ES is fragile in the presence of conflicting advantages, show an algebraic equivalence
between MVS and LSW under the standard (homogeneous) PPO surrogate, and finally state a provable condition under which
LSW is strictly preferable in practical pipelines that include per-objective preprocessing or adaptive, non-homogeneous
operations.
Lemma E.1(ES magnitude loss).LetA ω
t :=ω ⊤At andM LSW := Pd
i=1 ωi|A(i)
t |. Then
|Aω
t | ≤MLSW ,
with strict inequality whenever there existi, jwithA (i)
t A(j)
t <0andω i, ωj >0.
Proof.Immediate from the triangle inequality:
ω⊤At
 =

dX
i=1
ωiA(i)
t
 ≤
dX
i=1
ωi|A(i)
t |=M LSW .
Strictness follows because the triangle inequality is strict when at least two nonzero terms have opposite signs.
Proposition E.2(Conditional equivalence of MVS and LSW under homogeneous surrogate).Assume the PPO surrogate
evaluates each candidate term by multiplication with a scalar factor drawn from {ρt(θ),clip(ρ t(θ),1−ϵ,1 +ϵ)} , i.e.
the surrogate is homogeneous and linear in the advantage. Under this homogeneity hypothesis, the MVS and LSW actor
objectives are algebraically identical:
LMV S
actor (θ) =L LSW
actor (θ).
Proof sketch. For a fixed objective index i and given scalar multipliers ct(ρ)∈ {ρt(θ),clip(ρ t(θ),1−ϵ,1 +ϵ)} , the
per-objective MVS surrogate is
min
 
ct(ρ)ω iA(i)
t , c′
t(ρ)ω iA(i)
t

.
Becauseω i ≥0, the scalarω i factors out:
min
 
ct(ρ)ω iA(i)
t , c′
t(ρ)ω iA(i)
t

=ω i min
 
ct(ρ)A(i)
t , c′
t(ρ)A(i)
t

.
Summing overiyieldsL MV S
actor (θ) =L LSW
actor (θ), proving algebraic equivalence.
14
--- Page 15 ---
Decomposed, Diversity-Driven Policy Optimization
RemarkE.3.At first glance, MVS and LSW appear algebraically similar. Indeed, under the highly restrictive assumption of
a homogeneous surrogate with no per-objective preprocessing, they are equivalent. However, this assumption never holds in
practice: variance normalization, per-objective critics, and clipping introduce non-homogeneities that make the order of
operations critical. In these realistic settings, LSW uniquely preserves the full magnitude of the stabilized advantage signal,
while MVS prematurely dampens it.
Proposition E.4(Practical superiority of LSW under non-homogeneous per-objective processing).Suppose some per-
objective preprocessing operators Pi(·) are applied to advantages before the surrogate, wherePi is not positively homoge-
neous of degree 1 (i.e., ∃ri ̸= 1 such that Pi(αx) =α riPi(x) does not hold for all α >0). Then there exist advantages
{A(i)
t }and weights{ω i}for which
ωiPi(A(i)
t )̸=P i(ωiA(i)
t ),
and, in these cases, weightingafterstabilization (LSW) preserves a strictly larger stabilized contribution than weighting
beforestabilization (MVS).
Proof sketch. If Pi is linear and homogeneous of degree 1, then Pi(ωiA) =ω iPi(A) and no difference arises (cf. Proposi-
tion E.2). For any Pi that is nonlinear or homogeneous of degree ri ̸= 1, the order of scaling matters. For example, take
Pi(x) =|x| γ sign(x)(a toy nonlinearity with degreeγ). Then
Pi(ωiA) =ω γ
i |A|γ sign(A), ω iPi(A) =ω i|A|γ sign(A).
If 0< ωi <1 and γ <1, then ωγ
i > ωi, so |Pi(ωiA)|>|ω iPi(A)|. Thus there exist realistic preprocessing operators for
which applying ωi before preprocessing reduces the stabilized magnitude compared to applying ωi after preprocessing.
Many practical pipelines include variance normalization, adaptive per-objective clipping, or critic-dependent scaling, all of
which break degree-1 homogeneity; in these common cases LSW preserves larger stabilized signals than MVS.
Corollary E.5(Hierarchy of robustness).Combining Lemma E.1, Proposition E.2, and Proposition E.4 yields the claimed
robustness ordering:
LSW⪰MVS≻ES,
where ‘⪰‘ denotes practical superiority (LSW is at least as robust as MVS in the homogeneous surrogate and strictly more
robust when non-homogeneous per-objective processing is present), and ‘≻‘ indicates strict superiority over ES due to
avoidance of inter-objective advantage cancellation.
E.3. Implications
The above results give a precise mathematical basis for the design choice of LSW:
• Avoid cancellation:ES can drastically shrink or cancel learning signals when advantages conflict; Lemma E.1
quantifies this loss of magnitude.
• Equivalence under ideal surrogate:MVS and LSW are algebraically identical under a homogeneous PPO surrogate
(Proposition E.2), so any empirical gap is due to per-objective non-linearities or implementation-level choices.
• Practical preference for LSW:When pipelines include per-objective normalization, per-objective ratios, adaptive
clipping, or other non-homogeneous operators (common in practice), LSW preserves stabilized event magnitudes better
than MVS (Proposition E.4).
F. Theoretical Analysis of the Scaled Diversity Regularizer
In this section, we provide a formal argument that the scaled diversity regularizer enforces separation in policy space
proportional to separation in preference space, thereby preventing representational mode collapse.
Definition F.1(Representational Mode Collapse).A preference-conditioned policy πθ(a|s, ω)exhibitsmode collapse
if there exists a region in the preference simplex of non-zero measure where two distinct preference vectors, ωA ̸=ω B,
produce statistically indistinguishable action distributions for all states. Formally, for someδ=∥ω A −ω B∥1 >0,
Es∼dπ
h
DKL(πθ(·|s, ωA)∥π θ(·|s, ωB))
i
= 0,
whered π is the state visitation distribution.
15
--- Page 16 ---
Decomposed, Diversity-Driven Policy Optimization
Proposition F.2(Separation Induced by Diversity Regularizer).Let the actor objective be
Lactor(θ) =L policy(θ) +λ div Ldiversity(θ),
withλ div, α >0and
Ldiversity(θ) =E s,ω,ω′
h 
DKL(πθ(·|s, ω)∥πθ(·|s, ω′))−α∥ω−ω ′∥1
2i
.
Then any global minimizerπ θ∗ must satisfy
Es
h
DKL(πθ∗(·|s, ωA)∥π θ∗(·|s, ωB))
i
=α∥ω A −ω B∥1 ∀ω A, ωB.
In particular, for any ωA ̸=ω B, the induced KL divergence is strictly positive; thus, the optimal policy cannot exhibit mode
collapse.
Proof.The diversity loss is a nonnegative sum of squared terms. For each pair(ω A, ωB), the contribution is

Es[DKL(πθ(·|s, ωA)∥π θ(·|s, ωB))]−α∥ω A −ω B∥1
2
.
This quadratic term is minimized when the inner expression vanishes, i.e.,
Es[DKL(πθ(·|s, ωA)∥π θ(·|s, ωB))] =α∥ω A −ω B∥1.
Therefore, at any global minimizer θ∗ of Lactor, the condition holds for all preference pairs. If ∥ωA −ω B∥1 =δ >0 , the
target separation is αδ >0, so the KL divergence must also be strictly positive. Mode collapse (which implies KL = 0 for
some δ >0) cannot minimize the objective. This establishes that the scaled diversity regularizer enforces a diverse mapping
from preferences to behaviors.
Convexity and Expressiveness.While Proposition F.2 shows that the scaled diversity regularizer enforces preference-
proportional separation in policy space, it is important to emphasize that this separation islocal and realizable: the regularizer
does not impose global convexity on the Pareto front, nor does it force the learning procedure to fabricate behaviors that are
not supported by the environment.
The regularizer penalizes insufficient separation only when distinct behaviors are feasible; when the underlying environment
admits only a finite set of Pareto-optimal solutions, the RL objective dominates and the policy converges to these true
solutions, even if the resulting front is nonconvex. Thus, the diversity termencouragesdistinct solutions for distinct
preferences but does notrequirethe emergence of new policies beyond what the environment affords.
G. Theoretical Analysis of Convergence
We analyze the convergence behavior of preference-conditioned actor updates with the scaled diversity regularizer. We
first consider an idealized tabular setting, where convergence to stationary points can be established under exact gradients.
We then extend the analysis to the function-approximation regime, where stochastic approximation theory guarantees
convergence to stationary points under standard assumptions.
Theorem G.1(Convergence to Stationary Points in the Tabular Setting).Assume:
1. The environment is a finite MDP with bounded rewards and finite state and action spaces.
2. The policy is parameterized in tabular form, i.e., each state–preference pair (s, ω)has an independent probability
distribution over actions.
3. The exact expected actor objective J(π), including the scaled diversity regularizer, is available, and exact gradients
with respect toπcan be computed.
4. Gradient ascent is performed with a sufficiently small constant step size or a diminishing step-size schedule.
16
--- Page 17 ---
Decomposed, Diversity-Driven Policy Optimization
Then gradient ascent converges to the set of stationary points ofJ(π).
Proof sketch. In the tabular parameterization, the optimization variables are the policy probability vectors {π(·|s, ω)}, one
for each state–preference pair(s, ω). These variables lie in a product of probability simplices, which is compact.
The policy improvement term of the actor objective is linear inπ. The scaled diversity regularizer involves squared deviations
of expected KL divergences, which are generally nonconvex functions ofπ. As a result, the combined actor objective J(π)
is smooth but not necessarily concave.
Since J(π) is continuously differentiable on a compact domain, gradient ascent with exact gradients and sufficiently small
step sizes is guaranteed to converge to the set of stationary points of J(π). This follows from standard results on gradient
ascent for smooth nonconvex objectives on compact domains.
Therefore, while global optimality cannot be guaranteed due to nonconvexity, convergence to stationary points holds in the
tabular setting.
Theorem G.2(Convergence to Stationary Points with Function Approximation).Let J(θ) denote the expected actor
objective, including the scaled diversity regularizer, and assume:
1.J(θ)is continuously differentiable andL-smooth.
2. The stochastic gradient estimatorsˆgt are unbiased and have bounded variance:
E[ˆgt | Ft] =∇J(θ t),E∥ˆg t − ∇J(θt)∥2 ≤σ 2.
3. The step-sizes{η t}satisfy the Robbins–Monro conditions:
∞X
t=1
ηt =∞,
∞X
t=1
η2
t <∞.
4. The parameter sequence{θ t}remains in a compact set or is projected onto one.
Then
lim
t→∞
∥∇J(θt)∥= 0almost surely.
Proof sketch.The actor parameters are updated according to
θt+1 =θ t +η tˆgt,
whereˆgt is an unbiased stochastic estimator of∇J(θ t). Define the noise sequence
Mt+1 = ˆgt − ∇J(θt),
which forms a martingale difference sequence with bounded variance by assumption.
UnderL-smoothness ofJ, the associated mean ODE
˙θ=∇J(θ)
has Lipschitz continuous dynamics. The Robbins–Monro step-size conditions ensure diminishing noise influence, while
compactness of the parameter domain guarantees bounded iterates.
Standard stochastic approximation results imply that the iterates {θt} asymptotically track the mean ODE, and their limit set
is contained in the set of stationary points ofJ. Consequently,
lim
t→∞
∥∇J(θt)∥= 0almost surely.
17
--- Page 18 ---
Decomposed, Diversity-Driven Policy Optimization
Interpretation.The tabular result establishes convergence to stationary points under exact gradients, reflecting the inherent
nonconvexity introduced by the scaled diversity regularizer. In the function-approximation regime, convergence to stationary
points follows from standard stochastic approximation theory under smoothness and noise assumptions. These guarantees
are comparable to those available for modern policy gradient methods such as PPO and SAC, and provide a theoretical
foundation for the stability observed empirically.
H. Environment Descriptions
Minecart.A multi-objective task where an agent controls a cart in a 2D continuous environment. The state space is
70dimensional. The agent selects from a discrete action space (6 actions) to navigate the environment and mine for resources.
The reward is a 3-dimensional vector, with conflicting objectives for collecting two different types of ore while minimizing
fuel consumption. The agent must learn to navigate between different mining locations, creating a trade-off between the
types of ore collected and the fuel expended. The hypervolume reference point is[−1,−1,−200] and the γ used to calculate
the returns to construct the front is 0.99
Lunar-Lander-4D.A multi-objective version of the classic Lunar Lander control problem. The state space is 8-
dimensional (S ⊆R8), containing the lander’s position, velocity, angle, and leg contact information. The agent selects
from a 4-dimensional discrete action space (A) representing firing the main engine, the left or right orientation thrusters, or
doing nothing. The reward is a 4-dimensional vector, with separate components for the landing outcome (success or crash),
a distance-based shaping reward, main engine fuel cost, and side engine fuel cost. The hypervolume reference point is
[−101,−1001,−101,−101]and theγused to calculate the returns to construct the front is 0.99
Hopper-2D.A continuous-control task based on the Hopper-v5 environment, where a one-legged robot must learn a
trade-off between forward movement and jumping height. The observation space is 11-dimensional (S ⊆R11), capturing
joint angles and velocities, while the 3-dimensional continuous action space ( A ⊆R3) controls joint torques. The two
objectives are the agent’s forward velocity and its vertical displacement, both augmented with a small control cost. The
hypervolume reference point is[−100,−100]and theγused to calculate the returns to construct the front is 0.99.
Hopper-3D.An extension of MO-Hopper-2D with an explicit third objective: minimizing control cost. The agent must
now learn a three-way trade-off between forward velocity, jumping height, and energy efficiency, which is defined as the
negative squared magnitude of the action vector (−Pa2
i ). The observation space remains 11-dimensional and the action
space 3-dimensional. The hypervolume reference point is [−100,−100,−100] and the γ used to calculate the returns to
construct the front is 0.99.
Ant-2D.Based on the Ant-v5 robot, this continuous-control task involves a quadruped navigating a 2D plane. The state
space is 105-dimensional ( S ⊆R105), representing joint positions, velocities, and contact forces. The action space is
8-dimensional (A ⊆R8), controlling the torques at each leg joint. The 2-dimensional reward vector consists of the agent’s
x-velocity (vx) and y-velocity (vy). The hypervolume reference point is [−100,−100] and the γ used to calculate the returns
to construct the front is 0.99.
Ant-3D.An extension of MO-Ant-2D with an additional objective for control cost. The agent must optimize its x-velocity
and y-velocity while simultaneously minimizing the magnitude of applied joint torques (−2 Pa2
i ). The state space remains
105-dimensional and the action space 8-dimensional, but the objective space is now 3-dimensional. The hypervolume
reference point is[−100,−100,−100]and theγused to calculate the returns to construct the front is 0.99.
Humanoid-2D.Based on the Humanoid-v5 robot, this environment features one of the most complex state spaces in
common benchmarks, with 348 state dimensions (S ⊆R348) and a 17-dimensional continuous action space (A ⊆R17).
The task presents two highly conflicting objectives: maximizing forward velocity (vx) and minimizing energy consumed,
represented by a control cost penalty ( −10 Pa2
i ). The hypervolume reference point is [−100,−100] and the γ used to
calculate the returns to construct the front is 0.99.
Building-9D.A complex thermal control task for a large commercial building, featuring a 29-dimensional state space
(S ⊆R29) and a 23-dimensional continuous action space (A ⊆R23). The agent must manage the heating supply across
23 zones. The three core objectives (minimizing energy cost, temperature deviation, and power ramping) are calculated
18
--- Page 19 ---
Decomposed, Diversity-Driven Policy Optimization
independently for each of the building’s three floors, resulting in a challenging, high-dimensional 9-objective problem. The
hypervolume reference point is[0,0,0,0,0,0,0,0,0]and theγused to calculate the returns to construct the front is 1.
I. Experimental Details
The PPO specific hyperparameters are the following:
• Number of environments: 4
• Learning Rate: 0.0003
• Batch Size: 512
• Number of minibatches: 32
• Gamma: 0.995
• GAE lambda: 0.95
• Surrogate Clip Threshold: 0.2
• Entropy Loss coefficient: 0
• Value function loss coefficient: 0.5
• Normalize Advantages, Normalize Observations, Normalize rewards: True
• Max gradient Norm: 0.5
For the actor network, we initialized the final layer with logstd value of 0. For humanoid and ant benchmarks, the logstd
value was -1. We performed every experiment with 5 random seeds to find confidence intervals. In all cases, both actor and
critic networks had 2 hidden layers with 64 neurons in each layer. The activations were tanh, with the final layer having
no activation. Increasing the capacity of the network caused instability in learning. The KL divergence of the policy was
extremely high resulting in high policy entropy and it being unable to learn properly, which we attribute to overfitting. For
all experiments, the action diversity loss parameterλwas 0.01 andα= 1
We trained all baselines and D3PO on a Xeon Gold 6330 CPU, where every experiment was allotted 14 cores and 128Gb
RAM. The experiments did not use GPUs.
All baselines used the same number of environment interactions, network architecture size, and PPO parameters.
I.1. Reward Curves
Figure 3 presents the learning curves for all environments and objectives considered in our experiments. For each domain
(Hopper-2d, Hopper-3d, Ant-2d, Ant-3d and Humanoid-2d), we report the per-objective returns (Obj 1, Obj 2, . . . ) as well
as the overall return, which corresponds to the weighted combination of objectives used for policy optimization. Each
subfigure shows the mean return over training timesteps, with shaded regions indicating ±1 standard deviation across
multiple seeds. The per-objective curves illustrate how individual task components evolve during training, reflecting how the
policy balances different objectives. The overall return curves summarize the net performance achieved under the specified
weighting scheme. Together, these plots provide a comprehensive view of the learning dynamics for each environment and
demonstrate that the proposed method consistently improves both objective-specific and aggregated performance over time.
I.2. Statistical Testing Methodology
To evaluate the performance differences between D3PO and C-MORL across six benchmark environments (Ant-2d, Ant-3d,
Hopper-2d, Hopper-3d, Humanoid-2d, Building-9d), we performed a standardized statistical analysis consistent with
established deep reinforcement learning practice. Each algorithm was run across five independent random seeds per
environment, yielding per-seed values for three multi-objective metrics: hypervolume (HV; higher is better), expected utility
(EU; higher is better), and sparsity (SP; lower is better).
19
--- Page 20 ---
Decomposed, Diversity-Driven Policy Optimization
Table 7.Distributional diagnostics for D 3PO and C-MORL performance metrics. Shapiro–Wilk and Levene tests characterize normality
and variance properties; these diagnostics inform interpretation but donotdetermine the choice of statistical test. All significance testing
uses one-sided Welch’st-tests.
Env / Metric Shapiro W ShapiropLevene Stat LevenepNormal? Equal Var?
Ant-2d HV 0.967 0.839 0.812 0.396 Yes Yes
Ant-2d EU 0.952 0.710 1.221 0.292 Yes Yes
Ant-2d SP 0.941 0.602 1.884 0.180 Yes Yes
Ant-3d HV 0.882 0.284 6.914 0.016 Yes No
Ant-3d EU 0.901 0.355 5.788 0.025 Yes No
Ant-3d SP 0.791 0.081 8.322 0.011 Marginal No
Hopper-2d HV 0.926 0.507 2.448 0.131 Yes Yes
Hopper-2d EU 0.933 0.566 2.102 0.167 Yes Yes
Hopper-2d SP 0.912 0.398 4.554 0.041 Yes No
Hopper-3d HV 0.899 0.344 7.201 0.015 Yes No
Hopper-3d EU 0.871 0.242 6.772 0.018 Yes No
Hopper-3d SP 0.839 0.149 9.322 0.009 Marginal No
Humanoid-2d HV 0.961 0.787 1.332 0.265 Yes Yes
Humanoid-2d EU 0.947 0.662 1.441 0.239 Yes Yes
Humanoid-2d SP 0.712 0.022 16.551 0.002 No No
Building-9d HV 0.973 0.881 0.642 0.451 Yes Yes
Building-9d EU 0.968 0.844 0.723 0.423 Yes Yes
Building-9d SP 0.854 0.188 12.499 0.005 Yes No
Hypothesis testing.For each metric and environment, we conducted one-sided Welch’st-tests to assess whether D 3PO
significantly improves over C-MORL. Welch’s test is the standard choice for RL evaluations because it is robust to unequal
variances and small sample sizes. The alternative hypotheses were
H1 :µ D3PO > µC-MORL (HV , EU),
H1 :µ D3PO < µC-MORL (SP).
Diagnostics.We report Shapiro–Wilk normality tests and Levene variance tests to characterize distributional properties,
but these diagnostics were used only to interpret variance structure—not to select different statistical tests. Following RL
convention, Welch’st-test was used uniformly for all comparisons.
Effect sizes and confidence.We quantify effect magnitude using Hedges’ g, which provides a small-sample bias correction.
Additionally, we compute Welch 95% confidence intervals to capture the uncertainty around mean differences.
Multiple testing correction.Because 18 hypothesis tests were performed (six environments × three metrics), we applied
Holm–Bonferroni and Bonferroni corrections to control the family-wise error rate. Corrected p-values greater than 1 are
reported as 1.0.
Interpreting non-significant outcomes.Where statistical significance is not reached, we distinguish between (1) genuinely
small mean differences and (2) high variance that inflates standard errors. In several environments, C-MORL exhibits
substantial variance, especially in sparsity, resulting in large confidence intervals that obscure clear practical improvements
under D3PO (e.g., Humanoid-2d SP). Thus, non-significance in these cases reflects variance inflation rather than lack of
improvement.
I.2.1. RESULTS ANDANALYSIS
1. Strong and consistent improvements on Ant-2d.Across all three metrics, D 3PO demonstrates clear and statistically
significant gains on Ant-2d (HV:p= 0.00076 , EU: p= 0.0016 , SP: p= 1.8×10 −4), with very large effect sizes (|g|>2.4 ).
This environment showcases D3PO’s ability to reliably improve both reward quality and the structure of Pareto-optimal
solutions.
2. Robust sparsity improvements across most environments.D 3PO consistently achieves lower SP values in Ant-2d,
Ant-3d, Hopper-2d, Hopper-3d, and Building-9d. Several of these comparisons remain significant after correction, and many
20
